# Deep Learning Practice

This repository is a log of me exploring deep learning. Ideally I will update this repository with things I learned/did at different times. I'm currently also taking a Deep Learning course and working on few A.I. projects, which I may discuss here also. 

Pre February 2023- I've been really interested in artificial intelligence and specifically deep learning. I took an A.I. course last semester and speedran a basic ML course in Python (Scikit-Learn). I've seen most of the basics of deep learning (the general idea/major introductory models) at a high level, though haven't gone through things very thoroughly. Mostly watching different YouTube videos (3blue1brown's series on neural nets, different introductory lectures), looking at things on LinkedIn (figuring out the main ideas/use-cases of models like CNN, RNN, LSTM, GAN, Transformers, and others), and listening to podcasts (PracticalAI, Lex Friedman, and more to know more about the current state of the field). I'm currently working on a project to solve the Rubik's Cube with Deep Reinforcement Learning. 

February 2nd, 2023- Coded along with two basic projects from Keras' Code [Examples](https://keras.io/examples/). Specifically I created a MNIST CNN and also learned the basics of text classification. For my Cube project, I revised the adi.py file because I realized there was a big bug. 

February 3rd, 2023- Coded along with a timeseries classification project from Keras' Code [Examples](https://keras.io/examples/). Both today and yesterday have been exploring Tensorflow/Keras. For my Cube project I started doing the Group Theory section of my writeup, as I wrote a group-theoretic cube class that takes a bit to explain. 
  
February 4th, 2023- I watched MIT's 6.S191 [lecture](https://www.youtube.com/watch?v=QvkQ1B3FBqA) on RNNs and transformers. I also read a [blog](https://towardsdatascience.com/transformer-neural-network-step-by-step-breakdown-of-the-beast-b3e096dc857f) by Towards Data Science on the transformer architecture. Learned the basic ideas behind recurrent neural networks, backpropagation through time, self-attention, and the transformer architecture. For my Cube project I added tests (more specifically a ton of print statements...) to figure out why adi.py was not working. Turns out I messed up the get_scrambled_cube function, which really is not at all complicated.



  
