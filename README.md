# Deep Learning Practice

This repository is a log of me exploring deep learning. Ideally I will update this repository with things I learned/did at different times. I'm currently also taking a Deep Learning course and working on few A.I. projects, which I may discuss here also. 

Pre February 2023- I've been really interested in artificial intelligence and specifically deep learning. I took an A.I. course last semester and speedran a basic ML course in Python (Scikit-Learn). I've seen most of the basics of deep learning (the general idea/major introductory models) at a high level, though haven't gone through things very thoroughly. Mostly watching different YouTube videos (3blue1brown's series on neural nets, different introductory lectures), looking at things on LinkedIn (figuring out the main ideas/use-cases of models like CNN, RNN, LSTM, GAN, Transformers, and others), and listening to podcasts (PracticalAI, Lex Friedman, and more to know more about the current state of the field). I'm currently working on a project to solve the Rubik's Cube with Deep Reinforcement Learning. 

February 2nd, 2023- I coded along with two basic projects from Keras' Code [Examples](https://keras.io/examples/). Specifically I created a MNIST CNN and also learned the basics of text classification. For my Cube project, I revised the adi.py file because I realized there was a big bug. 

February 3rd, 2023- I coded along with a timeseries classification project from Keras' Code [Examples](https://keras.io/examples/). Both today and yesterday have been exploring Tensorflow/Keras. For my Cube project I started doing the Group Theory section of my writeup, as I wrote a group-theoretic cube class that takes a bit to explain. 
  
February 4th, 2023- I watched MIT's 6.S191 [lecture](https://www.youtube.com/watch?v=QvkQ1B3FBqA) on RNNs and transformers. I also read a [blog](https://towardsdatascience.com/transformer-neural-network-step-by-step-breakdown-of-the-beast-b3e096dc857f) by Towards Data Science on the transformer architecture. Learned the basic ideas behind recurrent neural networks, backpropagation through time, self-attention, and the transformer architecture. For my Cube project I added tests (more specifically a ton of print statements...) to figure out why adi.py was not working. Turns out I messed up the get_scrambled_cube function, which really is not at all complicated.

February 5th, 2023- I coded along with [this](https://www.kaggle.com/code/thebrownviking20/intro-to-recurrent-neural-networks-lstm-gru/notebook) Kaggle Notebook, which introduces RNNs, LSTMs, and GRUs. I didn't bother going too deep into the theory behind LSTMs and GRUs, but I just learned enough for a high level understanding/implementation. I also read [this](https://medium.com/deep-math-machine-learning-ai/chapter-10-1-deepnlp-lstm-long-short-term-memory-networks-with-math-21477f8e4235) medium article on the LSTM network. 

February 6th, 2023- I watched MIT's 6S.191 lectures on [CNNs](https://www.youtube.com/watch?v=uapdILWYTzE&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=3) and [Deep RL](youtube.com/watch?v=-WbN61qtTGQ&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=5). Both were semi-review; the CNN lecture dove a bit deeper than I had previously gone with convolutional neural networks, and the deep reinforcement learning lecture described policy gradient methods, which I had not previously seen in much detail. For my Cube project I began working on the Monte Carlo Tree Search class. 

February 9th, 2023- Didn't do too much the last few days (sick/lazy). Read a few short articles by [Run:ai](https://www.run.ai/) on GPUs for data parallelism and how to utilize with Keras. For my Cube project I continued working on the MCTS class (also created a fairly simple node class to support the MCTS). 

March 6th, 2023- Haven't written in this in a while... hopefully will start updating this soon. Since my last post I finished my Rubik's Cube project (at least a draft that I don't plan on returning to anytime soon). For now I've been going through different courses at school (a Deep Learning course and a Big Data Analysis course). I'm also beginning to relearn some fundamental topics in Probability (looking at [Harvard's Stat 110](https://projects.iq.harvard.edu/stat110/home) and Probability and Random Processes by Grimmett). I'm also working through [Berkeley's CS 285 Deep Reinforcement Learning course](https://rail.eecs.berkeley.edu/deeprlcourse/).

  
